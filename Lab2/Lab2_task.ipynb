{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045a8938-6de2-415c-a2d9-de759cf8dbde",
   "metadata": {},
   "source": [
    "# Завдання 1\n",
    "Використовуючи датасет з оглядами книжок з попередньої роботи, натренуйте word2vec модель з нуля, потім використайте вже натерновані моделі такі як word2vec, fasttext, doc2vec і порівняйте результати з натренованою власною моделлю.\n",
    "\n",
    "Візуалізуйте ембедінги і порівняйте близькість слів.\n",
    "### Word2Vec:\n",
    "- Розгляньте дві архітектури Word2Vec: Continuous Bag of Words (CBOW) та Skip-gram, висвітлюючи їхні відмінності та випадки використання.\n",
    "\n",
    "### Математичні основи:\n",
    "- Поясніть концепцію векторів слів та як Word2Vec використовує нейронну мережу для вивчення цих векторів.\n",
    "- Запровадьте цільові функції для CBOW та Skip-gram та концепцію негативного семплінгу або ієрархічного softmax.\n",
    "\n",
    "### Попередня обробка даних:\n",
    "- Токенізація: розбиття тексту на слова.\n",
    "- Побудова словника: ідентифікація унікальних слів.\n",
    "- Створення пар контексту: генерація пар вхід-ціль згідно обраної архітектури (CBOW або Skip-gram).\n",
    "- інші методи.\n",
    "\n",
    "### Реалізація моделі Word2Vec:\n",
    "- Поясніть архітектуру нейронної мережі, яка використовується в Word2Vec, зосереджуючись на вхідному шарі, прихованому шарі та вихідному шарі.\n",
    "- Обговоріть процес ініціалізації матриць ваг і роль softmax на вихідному шарі для прогнозування слів.\n",
    "\n",
    "### Тренування моделі:\n",
    "- Опишіть процес тренування, включаючи пряме поширення для обчислення прогнозів, loss функцію та бекпропагацію.\n",
    "- Поясніть важливість вибору відповідних гіперпараметрів, як-от швидкість навчання, розмір вікна, розмір вбудування тощо.\n",
    "\n",
    "### Вилучення векторів слів:\n",
    "- Після тренування моделі поясніть, як витягнути вектори слів з матриці ваг.\n",
    "- Обговоріть, як ці вектори можна використовувати для пошуку схожості слів, аналогій тощо."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a2bc3-65ba-47fe-bc66-b42d4fd464e5",
   "metadata": {},
   "source": [
    "Приклад тренування моделі word2vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2df14b1-138c-4eb9-9489-5aa95b4dbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "\n",
    "# Sample corpus\n",
    "corpus = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    # Convert the corpus to lowercase and split into words\n",
    "    words = corpus.lower().split()\n",
    "    # Count the frequency of each word in the corpus\n",
    "    word_count = collections.Counter(words)\n",
    "    return words, word_count\n",
    "\n",
    "def create_pairs(words, context_window):\n",
    "    # Create word pairs based on the context window\n",
    "    pairs = []\n",
    "    for i, word in enumerate(words):\n",
    "        # Iterate through the context window around the target word\n",
    "        for j in range(max(i - context_window, 0), min(i + context_window + 1, len(words))):\n",
    "            if i != j:\n",
    "                pairs.append((word, words[j]))  # Add the (target, context) pair\n",
    "    return pairs\n",
    "\n",
    "def get_word_index(word_count):\n",
    "    # Assign an index to each unique word\n",
    "    word_index = {}\n",
    "    for i, word in enumerate(word_count.keys()):\n",
    "        word_index[word] = i\n",
    "    return word_index\n",
    "\n",
    "def train_word2vec(pairs, word_count, dim, epochs, learning_rate):\n",
    "    # Initialize the weight matrices for the neural network\n",
    "    V = len(word_count)  # Vocabulary size\n",
    "    W = np.random.rand(V, dim)  # Input to hidden layer weights\n",
    "    W_prime = np.random.rand(dim, V)  # Hidden to output layer weights\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        for pair in pairs:\n",
    "            center_word, context_word = pair\n",
    "            center_index = word_index[center_word]  # Index of the center word\n",
    "            context_index = word_index[context_word]  # Index of the context word\n",
    "\n",
    "            # Forward pass: compute the hidden layer and output layer predictions\n",
    "            hidden_layer = W[center_index]\n",
    "            predicted = softmax(np.dot(hidden_layer, W_prime))\n",
    "\n",
    "            # Compute the loss: negative log likelihood\n",
    "            loss -= np.log(predicted[context_index])\n",
    "\n",
    "            # Backpropagation: compute gradients and update weights\n",
    "            dl_dy = predicted\n",
    "            dl_dy[context_index] -= 1  # Derivative of loss w.r.t. softmax output\n",
    "\n",
    "            dl_dw_prime = np.outer(hidden_layer, dl_dy)  # Gradient w.r.t. W_prime\n",
    "            dl_dhidden = np.dot(W_prime, dl_dy)  # Gradient w.r.t. hidden layer\n",
    "\n",
    "            W_prime -= learning_rate * dl_dw_prime  # Update W_prime\n",
    "            W[center_index] -= learning_rate * dl_dhidden  # Update W\n",
    "\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return W  # Return the input-to-hidden layer weights, which are the word embeddings\n",
    "\n",
    "def softmax(x):\n",
    "    # Compute softmax values for each set of scores in x\n",
    "    e_x = np.exp(x - np.max(x))  # Subtract max for numerical stability\n",
    "    return e_x / e_x.sum(axis=0)  # Divide by the sum to get probabilities\n",
    "\n",
    "# Preprocess corpus and create training pairs\n",
    "words, word_count = preprocess_corpus(corpus)\n",
    "pairs = create_pairs(words, 3)  # Create word pairs with a context window of size 3\n",
    "\n",
    "# Map words to indices\n",
    "word_index = get_word_index(word_count)\n",
    "\n",
    "# Train the model\n",
    "word_vectors = train_word2vec(pairs, word_count, dim=50, epochs=100, learning_rate=0.01)\n",
    "\n",
    "# After training, word_vectors can be used to look up embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f081b5c7-4889-4a01-8d8e-f50b294d7136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.73312377, -0.2243597 ,  0.763462  ,  0.72711357,  0.07438551,\n",
       "        0.17363124,  0.28799628,  0.668847  ,  0.28413874,  0.51918509,\n",
       "        0.3700634 ,  0.37579389,  0.23729036,  0.36275052,  0.81962296,\n",
       "        0.51279331,  0.17977212,  0.45761423,  0.89762095,  0.4826054 ,\n",
       "        0.25128048,  0.81143442,  0.19944879,  0.98341214,  0.20729798,\n",
       "        0.30121501,  0.62613067,  0.48557193,  0.55219873,  0.41594794,\n",
       "        0.87899377,  0.25090532,  0.074254  ,  0.40836686,  0.54932731,\n",
       "        0.01761935,  0.29535115,  0.20458697,  0.39046555,  0.89441631,\n",
       "        0.77064025,  1.02589797, -0.03780116,  0.3492056 ,  0.86108796,\n",
       "        0.1766345 ,  0.95802129,  0.01195978,  0.55145408, -0.14019507])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff9259c-2387-4bdd-a5a1-9eaf508126b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'quick':\n",
      "[ 0.55722498  0.69708446  0.36800627  0.84113167  0.92231125  0.44551605\n",
      "  0.35181475  0.14035737  0.82378431  0.96017812  0.53859522  0.71395414\n",
      "  0.35054537  1.01752151  0.04704904  0.99172569  0.75298303  0.66391169\n",
      "  0.85405183  0.12345592  0.3061029   0.330706    0.97442096  0.02329407\n",
      "  0.74522549  0.64826541  0.89582091 -0.13886917  0.75648919  0.77630262\n",
      "  0.09049278 -0.05852113  0.65630137  0.38190596  0.78689558 -0.05521722\n",
      " -0.02118902  0.52046586  0.80612118  0.30981224  0.93959901  0.21012012\n",
      "  0.29884303  0.79334215  0.1667451   0.58759007  0.73810778  0.10465658\n",
      "  0.44081445  0.72099184]\n",
      "Words closest to 'quick': ['fox', 'brown', 'lazy']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assume word_vectors is the trained word embeddings matrix from the Word2Vec model\n",
    "# word_index is a dictionary mapping words to their index in the word_vectors matrix\n",
    "\n",
    "# Function to get the vector for a specific word\n",
    "def get_word_vector(word, word_index, word_vectors):\n",
    "    index = word_index.get(word, None)\n",
    "    if index is not None:\n",
    "        return word_vectors[index]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to find the closest words to a given word\n",
    "def closest_words(word, word_index, word_vectors, n=5):\n",
    "    index = word_index.get(word, None)\n",
    "    if index is None:\n",
    "        return None\n",
    "    \n",
    "    # Compute cosine similarity between the word and all other words\n",
    "    similarities = cosine_similarity(word_vectors[index].reshape(1, -1), word_vectors).flatten()\n",
    "    \n",
    "    # Get the indices of the top n similar words\n",
    "    closest_indices = similarities.argsort()[-n-1:-1][::-1]  # Exclude the word itself\n",
    "    \n",
    "    # Map indices back to words\n",
    "    closest_words = [list(word_index.keys())[list(word_index.values()).index(i)] for i in closest_indices]\n",
    "    \n",
    "    return closest_words\n",
    "\n",
    "# Example usage:\n",
    "word = \"quick\"\n",
    "word_vector = get_word_vector(word, word_index, word_vectors)\n",
    "print(f\"Vector for '{word}':\\n{word_vector}\")\n",
    "\n",
    "# Find words closest to 'quick'\n",
    "closest = closest_words('quick', word_index, word_vectors, n=3)\n",
    "print(f\"Words closest to 'quick': {closest}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6c92dc6-e055-4f74-a737-71a55bf4c1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_embeddings(word_vectors, word_index):\n",
    "    # Since we have a very small number of samples, we set perplexity to a low value\n",
    "    # Perplexity should be less than the number of samples\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=5)  # Adjust perplexity here\n",
    "    vectors_2d = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for word, index in word_index.items():\n",
    "        x, y = vectors_2d[index]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(word, (x, y), textcoords=\"offset points\", xytext=(5, 2), ha='center')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "visualize_embeddings(word_vectors, word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8b651-29f5-42cb-963a-29b4630e2204",
   "metadata": {},
   "source": [
    "# Завдання 2\n",
    "Зробіть Fully Connected NN для передбачення рейтингу книжки. Використайте свою word2vec модель та порівняйте з використанням інших векторів (для порівняння використовуйте F1 метрику)\n",
    "\n",
    "- Поясніть структуру FFNN, яка складається з вхідного шару, одного або декількох прихованих шарів та вихідного шару.\n",
    "- Обговоріть функції активації, такі як ReLU для прихованих шарів та лінійну функцію активації для вихідного шару у випадку регресії.\n",
    "- Визначте кількість шарів та кількість нейронів у кожному шарі.\n",
    "- Для побудови мережі використовуйте numpy а потім pytorch. Порі\n",
    "- Після навчання оцініть продуктивність FFNN на валідаційному або тестовому наборі.\n",
    "- Використовуйте відповідні метрики, такі як MSE або середня абсолютна помилка (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ccc5d-ed2d-442f-9e4e-8ccdd969d0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ee80b3-169b-42a9-8375-4ea5378dccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 4286.654652517146\n",
      "Epoch: 10, Loss: 2.00000000078163\n",
      "Epoch: 20, Loss: 2.0\n",
      "Epoch: 30, Loss: 2.0\n",
      "Epoch: 40, Loss: 2.0\n",
      "Epoch: 50, Loss: 2.0\n",
      "Epoch: 60, Loss: 2.0\n",
      "Epoch: 70, Loss: 2.0\n",
      "Epoch: 80, Loss: 2.0\n",
      "Epoch: 90, Loss: 2.0\n",
      "Book 1: Actual Rating = 2, Predicted Rating = 3\n",
      "Book 2: Actual Rating = 1, Predicted Rating = 3\n",
      "Book 3: Actual Rating = 5, Predicted Rating = 3\n",
      "Book 4: Actual Rating = 5, Predicted Rating = 3\n",
      "Book 5: Actual Rating = 5, Predicted Rating = 3\n",
      "Book 6: Actual Rating = 3, Predicted Rating = 3\n",
      "Book 7: Actual Rating = 5, Predicted Rating = 3\n",
      "Book 8: Actual Rating = 2, Predicted Rating = 3\n",
      "Book 9: Actual Rating = 4, Predicted Rating = 3\n",
      "Book 10: Actual Rating = 4, Predicted Rating = 3\n",
      "Book 11: Actual Rating = 5, Predicted Rating = 3\n",
      "Book 12: Actual Rating = 4, Predicted Rating = 3\n",
      "Book 13: Actual Rating = 3, Predicted Rating = 3\n",
      "Book 14: Actual Rating = 4, Predicted Rating = 3\n",
      "Book 15: Actual Rating = 5, Predicted Rating = 3\n",
      "Book 16: Actual Rating = 1, Predicted Rating = 3\n",
      "Book 17: Actual Rating = 2, Predicted Rating = 3\n",
      "Book 18: Actual Rating = 4, Predicted Rating = 3\n",
      "Book 19: Actual Rating = 2, Predicted Rating = 3\n",
      "Book 20: Actual Rating = 5, Predicted Rating = 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming we have 100 books\n",
    "num_books = 100\n",
    "vector_dim = 50  # Dimensionality of the Word2Vec embeddings\n",
    "\n",
    "# Simulating averaged word vectors for book descriptions (dummy data)\n",
    "book_vectors = np.random.rand(num_books, vector_dim)\n",
    "\n",
    "# Simulating book ratings (targets) on a scale of 1 to 5\n",
    "book_ratings = np.random.randint(1, 6, size=(num_books, 1))\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "train_size = int(num_books * 0.8)\n",
    "X_train, X_test = book_vectors[:train_size], book_vectors[train_size:]\n",
    "y_train, y_test = book_ratings[:train_size], book_ratings[train_size:]\n",
    "\n",
    "# FFNN architecture from the previous example\n",
    "input_size = vector_dim\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(0)  # For reproducibility\n",
    "W1 = np.random.rand(input_size, hidden_size)\n",
    "b1 = np.random.rand(hidden_size)\n",
    "W2 = np.random.rand(hidden_size, output_size)\n",
    "b2 = np.random.rand(output_size)\n",
    "\n",
    "# Activation and loss functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "# Forward pass function\n",
    "def forward_pass(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    return Z2\n",
    "\n",
    "# Training the FFNN\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    predictions = forward_pass(X_train, W1, b1, W2, b2)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = mse_loss(y_train, predictions)\n",
    "    \n",
    "    # Backpropagation (simplified)\n",
    "    error = predictions - y_train\n",
    "    dW2 = np.dot(error.T, relu(np.dot(X_train, W1) + b1)).T\n",
    "    db2 = np.sum(error, axis=0)\n",
    "    dW1 = np.dot(X_train.T, np.dot(error, W2.T) * (relu(np.dot(X_train, W1) + b1) > 0))\n",
    "    db1 = np.sum(np.dot(error, W2.T) * (relu(np.dot(X_train, W1) + b1) > 0), axis=0)\n",
    "    \n",
    "    # Update weights and biases\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Predicting ratings for the test set\n",
    "test_predictions = forward_pass(X_test, W1, b1, W2, b2).flatten()\n",
    "test_predictions = np.round(test_predictions)  # Rounding predictions to get integer ratings\n",
    "\n",
    "# Comparing actual and predicted ratings for the test set\n",
    "for i in range(len(test_predictions)):\n",
    "    print(f\"Book {i+1}: Actual Rating = {y_test[i][0]}, Predicted Rating = {int(test_predictions[i])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "452d711d-c8c3-44e9-ac84-e70e86a54994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romankyslyi/opt/anaconda3/envs/transformers/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 10.309393882751465\n",
      "Epoch [11/100], Loss: 2.4357497692108154\n",
      "Epoch [21/100], Loss: 2.5755295753479004\n",
      "Epoch [31/100], Loss: 1.9827114343643188\n",
      "Epoch [41/100], Loss: 1.8942105770111084\n",
      "Epoch [51/100], Loss: 1.8655608892440796\n",
      "Epoch [61/100], Loss: 1.801476240158081\n",
      "Epoch [71/100], Loss: 1.755178451538086\n",
      "Epoch [81/100], Loss: 1.7109978199005127\n",
      "Epoch [91/100], Loss: 1.6619575023651123\n",
      "Book 1: Actual Rating = 2.0, Predicted Rating = 3.0\n",
      "Book 2: Actual Rating = 4.0, Predicted Rating = 3.0\n",
      "Book 3: Actual Rating = 3.0, Predicted Rating = 3.0\n",
      "Book 4: Actual Rating = 1.0, Predicted Rating = 3.0\n",
      "Book 5: Actual Rating = 3.0, Predicted Rating = 3.0\n",
      "Book 6: Actual Rating = 4.0, Predicted Rating = 3.0\n",
      "Book 7: Actual Rating = 3.0, Predicted Rating = 3.0\n",
      "Book 8: Actual Rating = 2.0, Predicted Rating = 3.0\n",
      "Book 9: Actual Rating = 5.0, Predicted Rating = 4.0\n",
      "Book 10: Actual Rating = 4.0, Predicted Rating = 3.0\n",
      "Book 11: Actual Rating = 5.0, Predicted Rating = 3.0\n",
      "Book 12: Actual Rating = 3.0, Predicted Rating = 3.0\n",
      "Book 13: Actual Rating = 3.0, Predicted Rating = 3.0\n",
      "Book 14: Actual Rating = 4.0, Predicted Rating = 3.0\n",
      "Book 15: Actual Rating = 3.0, Predicted Rating = 3.0\n",
      "Book 16: Actual Rating = 2.0, Predicted Rating = 3.0\n",
      "Book 17: Actual Rating = 4.0, Predicted Rating = 3.0\n",
      "Book 18: Actual Rating = 4.0, Predicted Rating = 3.0\n",
      "Book 19: Actual Rating = 5.0, Predicted Rating = 3.0\n",
      "Book 20: Actual Rating = 4.0, Predicted Rating = 4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming we have 100 books and each book is represented by a 50-dimensional averaged word vector\n",
    "num_books = 100\n",
    "vector_dim = 50\n",
    "\n",
    "# Creating dummy data: 100 book vectors and random ratings between 1 and 5\n",
    "book_vectors = torch.rand(num_books, vector_dim)\n",
    "book_ratings = torch.randint(1, 6, (num_books, 1), dtype=torch.float)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(num_books * 0.8)\n",
    "X_train, X_test = book_vectors[:train_size], book_vectors[train_size:]\n",
    "y_train, y_test = book_ratings[:train_size], book_ratings[train_size:]\n",
    "\n",
    "# Defining the FFNN using PyTorch\n",
    "class BookRatingPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BookRatingPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(vector_dim, 10)  # Input layer to hidden layer\n",
    "        self.relu = nn.ReLU()                 # Activation function\n",
    "        self.fc2 = nn.Linear(10, 1)           # Hidden layer to output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, define the loss function and the optimizer\n",
    "model = BookRatingPredictor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()   # Clearing the gradients\n",
    "    outputs = model(X_train)  # Forward pass\n",
    "    loss = criterion(outputs, y_train)  # Calculate the loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update the weights\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Testing the model\n",
    "model.eval()  # Evaluation mode\n",
    "with torch.no_grad():  # No need to track the gradients\n",
    "    predictions = model(X_test)\n",
    "    predictions = torch.round(predictions)  # Round predictions to get integer ratings\n",
    "\n",
    "# Compare actual and predicted ratings\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"Book {i+1}: Actual Rating = {y_test[i].item()}, Predicted Rating = {predictions[i].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde44f8-b274-4950-9ac3-f3d379cf53a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380cc3b8-6f80-442a-b59a-9ece99caf227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a904446e-1c46-4736-8f84-cae393343bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformers)",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
